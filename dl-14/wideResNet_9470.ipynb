{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet+wideResNet+MobileNet.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHbdF85zgeqI",
        "colab_type": "text"
      },
      "source": [
        "**ResNet在fashion-mnist数据集上的测试结果**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S81ls65rgQf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udvC3YdkhEq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJcfF7cFG4Uz",
        "colab_type": "text"
      },
      "source": [
        "# 新段落"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_LZUphiyRVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 定义加载数据集的函数\n",
        "def load_data_fashion_mnist(batch_size, root='./data', use_normalize=False, mean=None, std=None):\n",
        "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n",
        "\n",
        "    if use_normalize:\n",
        "        normalize = transforms.Normalize(mean=[mean], std=[std])\n",
        "        train_augs = transforms.Compose([transforms.RandomCrop(28, padding=2),\n",
        "                    transforms.CenterCrop(28),\n",
        "                    transforms.RandomResizedCrop(28),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    # transforms.RandomVerticalFlip(),\n",
        "                   \n",
        "                    transforms.ToTensor(), \n",
        "                    normalize])\n",
        "        test_augs = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "    else:\n",
        "        train_augs = transforms.Compose([transforms.ToTensor()])\n",
        "        test_augs = transforms.Compose([transforms.ToTensor()])\n",
        "    \n",
        "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=train_augs)\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=test_augs)\n",
        "    if sys.platform.startswith('win'):\n",
        "        num_workers = 0  # 0表示不用额外的进程来加速读取数据\n",
        "    else:\n",
        "        num_workers = 4\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_iter, test_iter\n",
        "\n",
        "\n",
        "print('计算数据集均值标准差')\n",
        "batch_size = 100  \n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size, root='./data', use_normalize=False)\n",
        "# 求整个数据集的均值\n",
        "temp_sum = 0\n",
        "cnt = 0\n",
        "for X, y in train_iter:\n",
        "    if y.shape[0] != batch_size:\n",
        "        break   # 最后一个batch不足batch_size,这里就忽略了\n",
        "    channel_mean = torch.mean(X, dim=(0,2,3))  # 按channel求均值(不过这里只有1个channel)\n",
        "    cnt += 1   # cnt记录的是batch的个数，不是图像\n",
        "    temp_sum += channel_mean[0].item()\n",
        "dataset_global_mean = temp_sum / cnt\n",
        "print('整个数据集的像素均值:{}'.format(dataset_global_mean))\n",
        "# 求整个数据集的标准差\n",
        "cnt = 0\n",
        "temp_sum = 0\n",
        "for X, y in train_iter:\n",
        "    if y.shape[0] != batch_size:\n",
        "        break   # 最后一个batch不足batch_size,这里就忽略了\n",
        "    residual = (X - dataset_global_mean) ** 2\n",
        "    channel_var_mean = torch.mean(residual, dim=(0,2,3))  \n",
        "    cnt += 1   # cnt记录的是batch的个数，不是图像\n",
        "    temp_sum += math.sqrt(channel_var_mean[0].item())\n",
        "dataset_global_std = temp_sum / cnt\n",
        "print('整个数据集的像素标准差:{}'.format(dataset_global_std))\n",
        "\n",
        "\n",
        "# 重新获取应用了归一化的数据集迭代器\n",
        "batch_size = 100  # 改大一点 跑得更快\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size, root='./data', use_normalize=True,\n",
        "                        mean = dataset_global_mean, std = dataset_global_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kj4MSBJyegh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 计算准确率\n",
        "def compute_accuracy(loader, net, device):\n",
        "    total_accu = 0.0\n",
        "    num = 0\n",
        "\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = net.forward(inputs)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total_accu += torch.mean((predicted == labels).float()).item()\n",
        "        num += 1\n",
        "    return total_accu / num\n",
        "\n",
        "# 定义训练函数\n",
        "def train_model(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
        "    train_acc_list = list()\n",
        "    test_acc_list = list()\n",
        "    loss_list = list()\n",
        "    \n",
        "    net = net.to(device)\n",
        "    print(\"training on \", device)\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    best_test_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            optimizer.zero_grad()\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            train_l_sum += l.cpu().item()\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
        "            n += y.shape[0]\n",
        "            batch_count += 1\n",
        "        \n",
        "        train_acc = compute_accuracy(train_iter, net, device)\n",
        "        test_acc = compute_accuracy(test_iter, net, device)\n",
        "        loss_list.append(train_l_sum / batch_count)   # 便于后续可视化看看损失函数值\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        \n",
        "        print('epoch %d, loss %.4f, train acc %.4f, test acc %.4f, time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
        "        if test_acc > best_test_acc:\n",
        "            print('find best! save at model/best.pth')\n",
        "            best_test_acc = test_acc\n",
        "            torch.save(net.state_dict(), 'model/best.pth')\n",
        "            \n",
        "    return loss_list, train_acc_list, test_acc_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zPyEi2KyliY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                               padding=0, bias=False) or None\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(1, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)   # 将原来的3通道改为1通道\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 4)   # 将原来的8改为4\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuue04Ivy3sK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('训练...')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "lr, num_epochs = 0.01, 250\n",
        "net = WideResNet(28,10,4,0.0).to(device)  # 40层，输出10类别  宽度为4  dropout = 0    在数据量小的情况下 不加dropout更好 相较于上次，这次加了个上下翻转 进行数据增强\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)   # 使用SGD效果更好\n",
        "loss_list,train_acc_list,test_acc_list = train_model(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gf7wKHTR0S2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('加载最优模型')\n",
        "net.load_state_dict(torch.load('best.pth'))\n",
        "net = net.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKR1vsqOy4c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('加载最优模型')\n",
        "net.load_state_dict(torch.load('model/best.pth'))\n",
        "net = net.to(device)\n",
        "\n",
        "\n",
        "print('inference测试集')\n",
        "net.eval() \n",
        "id = 0\n",
        "preds_list = []\n",
        "with torch.no_grad():\n",
        "    for X, y in test_iter:\n",
        "        batch_pred = list(net(X.to(device)).argmax(dim=1).cpu().numpy())\n",
        "        for y_pred in batch_pred:\n",
        "            preds_list.append((id, y_pred))\n",
        "            id += 1\n",
        "            \n",
        "print('生成提交结果文件')\n",
        "with open('submission_ResNet_01.csv', 'w') as f:\n",
        "    f.write('ID,Prediction\\n')\n",
        "    for id, pred in preds_list:\n",
        "        f.write('{},{}\\n'.format(id, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xweMFyOUR3hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}